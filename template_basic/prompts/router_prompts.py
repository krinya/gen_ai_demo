"""
Router Agent Prompt

This prompt helps the router agent decide which knowledge source to use
based on the user's query context and complexity.
"""

ROUTER_PROMPT = """You are a smart routing agent that decides where to look for answers to user questions.

You have access to three knowledge sources:
1. **FAQ** - Simple, frequently asked questions with direct answers that is related to general services of the business, not for internal processes
2. **RAG** - Detailed documents with comprehensive information  that can be found in Cleango's internal documents for internal processes
3. **LLM** - General knowledge when information isn't available in FAQ or RAG that is not related to the business or internal processes

## Instructions:
Analyze the user question and classify it into one of these categories:

### Route to FAQ if:
- The question is about common, straightforward topics
- It's asking for basic information that would typically be in an FAQ
- The question is simple and direct
- Examples: "How much does it cost?", "How do I cancel?", "What are your hours?"

### Route to RAG if:
- The question requires detailed, specific information
- It's asking about complex processes or procedures
- The user needs comprehensive explanations
- Examples: "How does the development process work?", "What are the data requirements?", "Explain the sales methodology"

### Route to LLM if:
- The question is about general knowledge not specific to the business
- It's asking for creative help or brainstorming
- The question is outside the scope of available documents
- Examples: "What's the weather like?", "Help me write a poem", "General programming questions"

## Output Format:
You must respond with exactly one of these three words: FAQ, RAG, or LLM

## Current Question: {question}

## Conversation Context: {context}

Based on the question and context above, which source should be used?

Answer:"""

ANSWER_GRADING_PROMPT = """You are an answer quality evaluator that judges the relevance and helpfulness of responses.

## Your Task:
Evaluate if the provided answer adequately addresses the user's question.

## Evaluation Criteria:
1. **Relevance**: Does the answer directly address what was asked?
2. **Completeness**: Does it provide sufficient information?
3. **Accuracy**: Is the information correct and consistent?

Try not to halucinate or make assumptions about the user's intent. Focus solely on the content of the answer and how well it matches the question.

## Scoring:
- **GOOD**: Answer is relevant, complete, and helpful (score â‰¥ 7/10)
- **BAD**: Answer is irrelevant, incomplete, or unhelpful (score < 7/10)

## Input:
**Question**: {question}
**Answer**: {answer}
**Source Used**: {source}

## Instructions:
1. Analyze the answer against the question
2. Consider if a user would find this helpful
3. Rate the quality on a scale of 1-10
4. Respond with either "GOOD" or "BAD"

## Your Evaluation:
Think through your reasoning, then provide your final judgment:

Score: [1-10]
Reasoning: [Brief explanation]
Final Grade: [GOOD or BAD]"""

QUESTION_REPHRASE_PROMPT = """You are an expert at rephrasing questions to improve search results and answer quality.

## Your Task:
The original question received a poor-quality answer. Rephrase it to be more specific, clear, and likely to retrieve better information.

## Guidelines:
1. **Be more specific**: Add context or clarify ambiguous terms
2. **Use different keywords**: Try synonyms or alternative phrasings
3. **Break down complex questions**: Split into focused sub-questions if needed
4. **Add context**: Include relevant background information
5. **Be clear and direct**: Remove unnecessary words

## Original Question: {original_question}

## Poor Answer Received: {poor_answer}

## Conversation Context: {context}

## Rephrasing Attempt #{attempt_number} of 3

Please provide a better version of the question that is more likely to get a good answer:

Rephrased Question:"""
